{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0428766f",
   "metadata": {
    "papermill": {
     "duration": 0.004584,
     "end_time": "2025-08-13T05:47:56.561340",
     "exception": false,
     "start_time": "2025-08-13T05:47:56.556756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7064686e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:47:56.570206Z",
     "iopub.status.busy": "2025-08-13T05:47:56.569981Z",
     "iopub.status.idle": "2025-08-13T05:48:24.188375Z",
     "shell.execute_reply": "2025-08-13T05:48:24.187637Z"
    },
    "papermill": {
     "duration": 27.624296,
     "end_time": "2025-08-13T05:48:24.189532",
     "exception": false,
     "start_time": "2025-08-13T05:47:56.565236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 05:48:00.304707: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755064080.472362      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755064080.520330      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ imports ready · tensorflow 2.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755064091.646461      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1755064091.647139      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ INFERENCE MODE – loading artefacts from /kaggle/input/cmi-d-111\n",
      " Loading models for ensemble inference...\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_0.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_1.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_2.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_3.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_4.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_5.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_6.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_7.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_8.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/D-111_9.h5\n",
      "--------------------------------------------------\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_0.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_1.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_2.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_3.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_4.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_5.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_6.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_7.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_8.h5\n",
      ">>>LoadModel>>> /kaggle/input/cmi-d-111/v0629_9.h5\n",
      "--------------------------------------------------\n",
      "[INFO]NumUseModels:20\n"
     ]
    }
   ],
   "source": [
    "import os, json, joblib, numpy as np, pandas as pd\n",
    "import random\n",
    "from pathlib import Path\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.utils import Sequence, to_categorical, pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, BatchNormalization, Activation, add, MaxPooling1D, Dropout,\n",
    "    Bidirectional, LSTM, GlobalAveragePooling1D, Dense, Multiply, Reshape,\n",
    "    Lambda, Concatenate, GRU, GaussianNoise\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.experimental.numpy.random.seed(seed)\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "seed_everything(seed=42)\n",
    "\n",
    "TRAIN = False\n",
    "RAW_DIR = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\n",
    "PRETRAINED_DIR = Path(\"/kaggle/input/cmi-d-111\")\n",
    "EXPORT_DIR = Path(\"./\")\n",
    "BATCH_SIZE = 64\n",
    "PAD_PERCENTILE = 95\n",
    "LR_INIT = 5e-4\n",
    "WD = 3e-3\n",
    "MIXUP_ALPHA = 0.4\n",
    "EPOCHS = 160\n",
    "PATIENCE = 40\n",
    "print(\"▶ imports ready · tensorflow\", tf.__version__)\n",
    "\n",
    "def time_sum(x):\n",
    "    return K.sum(x, axis=1)\n",
    "def squeeze_last_axis(x):\n",
    "    return tf.squeeze(x, axis=-1)\n",
    "def expand_last_axis(x):\n",
    "    return tf.expand_dims(x, axis=-1)\n",
    "def se_block(x, reduction=8):\n",
    "    ch = x.shape[-1]\n",
    "    se = GlobalAveragePooling1D()(x)\n",
    "    se = Dense(ch // reduction, activation='relu')(se)\n",
    "    se = Dense(ch, activation='sigmoid')(se)\n",
    "    se = Reshape((1, ch))(se)\n",
    "    return Multiply()([x, se])\n",
    "    \n",
    "def residual_se_cnn_block(x, filters, kernel_size, pool_size=2, drop=0.3, wd=1e-4):\n",
    "    shortcut = x\n",
    "    for _ in range(2):\n",
    "        x = Conv1D(filters, kernel_size, padding='same', use_bias=False,\n",
    "                   kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "    x = se_block(x)\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, 1, padding='same', use_bias=False,\n",
    "                          kernel_regularizer=l2(wd))(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    x = add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(pool_size)(x)\n",
    "    x = Dropout(drop)(x)\n",
    "    return x\n",
    "    \n",
    "def attention_layer(inputs):\n",
    "    score = Dense(1, activation='tanh')(inputs)\n",
    "    score = Lambda(squeeze_last_axis)(score)\n",
    "    weights = Activation('softmax')(score)\n",
    "    weights = Lambda(expand_last_axis)(weights)\n",
    "    context = Multiply()([inputs, weights])\n",
    "    context = Lambda(time_sum)(context)\n",
    "    return context\n",
    "    \n",
    "def preprocess_sequence(df_seq: pd.DataFrame, feature_cols: list[str], scaler: StandardScaler):\n",
    "    mat = df_seq[feature_cols].ffill().bfill().fillna(0).values\n",
    "    return scaler.transform(mat).astype('float32')\n",
    "    \n",
    "class MixupGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size, alpha=0.2):\n",
    "        self.X, self.y = X, y\n",
    "        self.batch = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.indices = np.arange(len(X))\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch))\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i*self.batch:(i+1)*self.batch]\n",
    "        Xb, yb = self.X[idx], self.y[idx]\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        perm = np.random.permutation(len(Xb))\n",
    "        X_mix = lam * Xb + (1-lam) * Xb[perm]\n",
    "        y_mix = lam * yb + (1-lam) * yb[perm]\n",
    "        return X_mix, y_mix\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "        \n",
    "def remove_gravity_from_acc(acc_data, rot_data):\n",
    "    if isinstance(acc_data, pd.DataFrame):\n",
    "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    else:\n",
    "        acc_values = acc_data\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "    for i in range(num_samples):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i, :] = acc_values[i, :]\n",
    "            continue\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "             linear_accel[i, :] = acc_values[i, :]\n",
    "    return linear_accel\n",
    "    \n",
    "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): \n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "    for i in range(num_samples - 1):\n",
    "        q_t = quat_values[i]\n",
    "        q_t_plus_dt = quat_values[i+1]\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
    "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
    "            continue\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except ValueError:\n",
    "            pass\n",
    "           \n",
    "    return angular_vel\n",
    "   \n",
    "def calculate_angular_distance(rot_data):\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_dist = np.zeros(num_samples)\n",
    "    for i in range(num_samples - 1):\n",
    "        q1 = quat_values[i]\n",
    "        q2 = quat_values[i+1]\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
    "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
    "            angular_dist[i] = 0 \n",
    "            continue\n",
    "        try:\n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "            angular_dist[i] = angle\n",
    "        except ValueError:\n",
    "            angular_dist[i] = 0 \n",
    "            pass\n",
    "           \n",
    "    return angular_dist\n",
    "    \n",
    "def build_two_branch_model(pad_len, imu_dim, tof_dim, n_classes, wd=1e-4):\n",
    "    inp = Input(shape=(pad_len, imu_dim+tof_dim))\n",
    "    imu = Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "    tof = Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "    x1 = residual_se_cnn_block(imu, 64, 3, drop=0.1, wd=wd)\n",
    "    x1 = residual_se_cnn_block(x1, 128, 5, drop=0.1, wd=wd)\n",
    "    x2 = Conv1D(64, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(tof)\n",
    "    x2 = BatchNormalization()(x2); x2 = Activation('relu')(x2)\n",
    "    x2 = MaxPooling1D(2)(x2); x2 = Dropout(0.2)(x2)\n",
    "    x2 = Conv1D(128, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x2)\n",
    "    x2 = BatchNormalization()(x2); x2 = Activation('relu')(x2)\n",
    "    x2 = MaxPooling1D(2)(x2); x2 = Dropout(0.2)(x2)\n",
    "    merged = Concatenate()([x1, x2])\n",
    "    xa = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n",
    "    xb = Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n",
    "    xc = GaussianNoise(0.09)(merged)\n",
    "    xc = Dense(16, activation='elu')(xc)\n",
    "    x = Concatenate()([xa, xb, xc])\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = attention_layer(x)\n",
    "    for units, drop in [(256, 0.5), (128, 0.3)]:\n",
    "        x = Dense(units, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x); x = Activation('relu')(x)\n",
    "        x = Dropout(drop)(x)\n",
    "    out = Dense(n_classes, activation='softmax', kernel_regularizer=l2(wd))(x)\n",
    "    return Model(inp, out)\n",
    "    \n",
    "tmp_model = build_two_branch_model(127,7,325,18)\n",
    "print(\"▶ INFERENCE MODE – loading artefacts from\", PRETRAINED_DIR)\n",
    "final_feature_cols = np.load(PRETRAINED_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "pad_len = int(np.load(PRETRAINED_DIR / \"sequence_maxlen.npy\"))\n",
    "scaler = joblib.load(PRETRAINED_DIR / \"scaler.pkl\")\n",
    "gesture_classes = np.load(PRETRAINED_DIR / \"gesture_classes.npy\", allow_pickle=True)\n",
    "custom_objs = {\n",
    "    'time_sum': time_sum, 'squeeze_last_axis': squeeze_last_axis, 'expand_last_axis': expand_last_axis,\n",
    "    'se_block': se_block, 'residual_se_cnn_block': residual_se_cnn_block, 'attention_layer': attention_layer,\n",
    "}\n",
    "models1 = []\n",
    "print(f\" Loading models for ensemble inference...\")\n",
    "for fold in range(10):\n",
    "    MODEL_DIR = \"/kaggle/input/cmi-d-111\"\n",
    "    model_path = f\"{MODEL_DIR}/D-111_{fold}.h5\"\n",
    "    print(\">>>LoadModel>>>\",model_path)\n",
    "    model = load_model(model_path, compile=False, custom_objects=custom_objs)\n",
    "    models1.append(model)\n",
    "print(\"-\"*50)\n",
    "for fold in range(10):\n",
    "    MODEL_DIR = \"/kaggle/input/cmi-d-111\"\n",
    "    model_path = f\"{MODEL_DIR}/v0629_{fold}.h5\"\n",
    "    print(\">>>LoadModel>>>\",model_path)\n",
    "    model = load_model(model_path, compile=False, custom_objects=custom_objs)\n",
    "    models1.append(model)\n",
    "print(\"-\"*50)\n",
    "print(f\"[INFO]NumUseModels:{len(models1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "859a074b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:48:24.199967Z",
     "iopub.status.busy": "2025-08-13T05:48:24.199520Z",
     "iopub.status.idle": "2025-08-13T05:48:24.207702Z",
     "shell.execute_reply": "2025-08-13T05:48:24.206967Z"
    },
    "papermill": {
     "duration": 0.014406,
     "end_time": "2025-08-13T05:48:24.208781",
     "exception": false,
     "start_time": "2025-08-13T05:48:24.194375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_1\n",
    "def predict1(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    df_seq = sequence.to_pandas()\n",
    "    linear_accel = remove_gravity_from_acc(df_seq, df_seq)\n",
    "    df_seq['linear_acc_x'], df_seq['linear_acc_y'], df_seq['linear_acc_z'] = linear_accel[:, 0], linear_accel[:, 1], linear_accel[:, 2]\n",
    "    df_seq['linear_acc_mag'] = np.sqrt(df_seq['linear_acc_x']**2 + df_seq['linear_acc_y']**2 + df_seq['linear_acc_z']**2)\n",
    "    df_seq['linear_acc_mag_jerk'] = df_seq['linear_acc_mag'].diff().fillna(0)\n",
    "    angular_vel = calculate_angular_velocity_from_quat(df_seq)\n",
    "    df_seq['angular_vel_x'], df_seq['angular_vel_y'], df_seq['angular_vel_z'] = angular_vel[:, 0], angular_vel[:, 1], angular_vel[:, 2]\n",
    "    df_seq['angular_distance'] = calculate_angular_distance(df_seq)\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]; tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n",
    "        df_seq[f'tof_{i}_mean'], df_seq[f'tof_{i}_std'], df_seq[f'tof_{i}_min'], df_seq[f'tof_{i}_max'] = tof_data.mean(axis=1), tof_data.std(axis=1), tof_data.min(axis=1), tof_data.max(axis=1)\n",
    "        \n",
    "    mat_unscaled = df_seq[final_feature_cols].ffill().bfill().fillna(0).values.astype('float32')\n",
    "    mat_scaled = scaler.transform(mat_unscaled)\n",
    "    pad_input = pad_sequences([mat_scaled], maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n",
    "    all_preds = [model.predict(pad_input, verbose=0)[0] for model in models1] \n",
    "    avg_pred = np.mean(all_preds, axis=0)\n",
    "    return avg_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821f7fa",
   "metadata": {
    "papermill": {
     "duration": 0.004136,
     "end_time": "2025-08-13T05:48:24.217362",
     "exception": false,
     "start_time": "2025-08-13T05:48:24.213226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e55bffee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:48:24.227515Z",
     "iopub.status.busy": "2025-08-13T05:48:24.227283Z",
     "iopub.status.idle": "2025-08-13T05:51:02.800548Z",
     "shell.execute_reply": "2025-08-13T05:51:02.799923Z"
    },
    "papermill": {
     "duration": 158.580184,
     "end_time": "2025-08-13T05:51:02.801984",
     "exception": false,
     "start_time": "2025-08-13T05:48:24.221800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have precomputed, skip compute.\n",
      "\n",
      "Cross-validation fold statistics:\n",
      "\n",
      "Fold 1:\n",
      "Category                                           Training Set Validation Set\n",
      "Above ear - pull hair                              511        127       \n",
      "Cheek - pinch skin                                 509        128       \n",
      "Drink from bottle/cup                              129        32        \n",
      "Eyebrow - pull hair                                510        128       \n",
      "Eyelash - pull hair                                512        128       \n",
      "Feel around in tray and pull out an object         129        32        \n",
      "Forehead - pull hairline                           512        128       \n",
      "Forehead - scratch                                 512        128       \n",
      "Glasses on/off                                     128        33        \n",
      "Neck - pinch skin                                  512        128       \n",
      "Neck - scratch                                     512        128       \n",
      "Pinch knee/leg skin                                129        32        \n",
      "Pull air toward your face                          381        96        \n",
      "Scratch knee/leg skin                              129        32        \n",
      "Text on phone                                      512        128       \n",
      "Wave hello                                         382        96        \n",
      "Write name in air                                  382        95        \n",
      "Write name on leg                                  129        32        \n",
      "\n",
      "Fold 2:\n",
      "Category                                           Training Set Validation Set\n",
      "Above ear - pull hair                              511        127       \n",
      "Cheek - pinch skin                                 509        128       \n",
      "Drink from bottle/cup                              129        32        \n",
      "Eyebrow - pull hair                                510        128       \n",
      "Eyelash - pull hair                                512        128       \n",
      "Feel around in tray and pull out an object         129        32        \n",
      "Forehead - pull hairline                           512        128       \n",
      "Forehead - scratch                                 512        128       \n",
      "Glasses on/off                                     129        32        \n",
      "Neck - pinch skin                                  512        128       \n",
      "Neck - scratch                                     512        128       \n",
      "Pinch knee/leg skin                                129        32        \n",
      "Pull air toward your face                          381        96        \n",
      "Scratch knee/leg skin                              129        32        \n",
      "Text on phone                                      512        128       \n",
      "Wave hello                                         382        96        \n",
      "Write name in air                                  382        95        \n",
      "Write name on leg                                  129        32        \n",
      "\n",
      "Fold 3:\n",
      "Category                                           Training Set Validation Set\n",
      "Above ear - pull hair                              510        128       \n",
      "Cheek - pinch skin                                 510        127       \n",
      "Drink from bottle/cup                              129        32        \n",
      "Eyebrow - pull hair                                511        127       \n",
      "Eyelash - pull hair                                512        128       \n",
      "Feel around in tray and pull out an object         129        32        \n",
      "Forehead - pull hairline                           512        128       \n",
      "Forehead - scratch                                 512        128       \n",
      "Glasses on/off                                     129        32        \n",
      "Neck - pinch skin                                  512        128       \n",
      "Neck - scratch                                     512        128       \n",
      "Pinch knee/leg skin                                128        33        \n",
      "Pull air toward your face                          382        95        \n",
      "Scratch knee/leg skin                              129        32        \n",
      "Text on phone                                      512        128       \n",
      "Wave hello                                         382        96        \n",
      "Write name in air                                  382        95        \n",
      "Write name on leg                                  128        33        \n",
      "\n",
      "Fold 4:\n",
      "Category                                           Training Set Validation Set\n",
      "Above ear - pull hair                              510        128       \n",
      "Cheek - pinch skin                                 510        127       \n",
      "Drink from bottle/cup                              129        32        \n",
      "Eyebrow - pull hair                                511        127       \n",
      "Eyelash - pull hair                                512        128       \n",
      "Feel around in tray and pull out an object         128        33        \n",
      "Forehead - pull hairline                           512        128       \n",
      "Forehead - scratch                                 512        128       \n",
      "Glasses on/off                                     129        32        \n",
      "Neck - pinch skin                                  512        128       \n",
      "Neck - scratch                                     512        128       \n",
      "Pinch knee/leg skin                                129        32        \n",
      "Pull air toward your face                          382        95        \n",
      "Scratch knee/leg skin                              128        33        \n",
      "Text on phone                                      512        128       \n",
      "Wave hello                                         383        95        \n",
      "Write name in air                                  381        96        \n",
      "Write name on leg                                  129        32        \n",
      "\n",
      "Fold 5:\n",
      "Category                                           Training Set Validation Set\n",
      "Above ear - pull hair                              510        128       \n",
      "Cheek - pinch skin                                 510        127       \n",
      "Drink from bottle/cup                              128        33        \n",
      "Eyebrow - pull hair                                510        128       \n",
      "Eyelash - pull hair                                512        128       \n",
      "Feel around in tray and pull out an object         129        32        \n",
      "Forehead - pull hairline                           512        128       \n",
      "Forehead - scratch                                 512        128       \n",
      "Glasses on/off                                     129        32        \n",
      "Neck - pinch skin                                  512        128       \n",
      "Neck - scratch                                     512        128       \n",
      "Pinch knee/leg skin                                129        32        \n",
      "Pull air toward your face                          382        95        \n",
      "Scratch knee/leg skin                              129        32        \n",
      "Text on phone                                      512        128       \n",
      "Wave hello                                         383        95        \n",
      "Write name in air                                  381        96        \n",
      "Write name on leg                                  129        32        \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.amp import autocast\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from transformers import BertConfig, BertModel\n",
    "def remove_gravity_from_acc(acc_data, rot_data):\n",
    "    if isinstance(acc_data, pd.DataFrame):\n",
    "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    else:\n",
    "        acc_values = acc_data\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "    for i in range(num_samples):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i, :] = acc_values[i, :]\n",
    "            continue\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "             linear_accel[i, :] = acc_values[i, :]\n",
    "    return linear_accel\n",
    "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200):\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "    for i in range(num_samples - 1):\n",
    "        q_t = quat_values[i]\n",
    "        q_t_plus_dt = quat_values[i+1]\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
    "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
    "            continue\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return angular_vel\n",
    "def calculate_angular_distance(rot_data):\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_dist = np.zeros(num_samples)\n",
    "    for i in range(num_samples - 1):\n",
    "        q1 = quat_values[i]\n",
    "        q2 = quat_values[i+1]\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
    "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
    "            angular_dist[i] = 0\n",
    "            continue\n",
    "        try:\n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "            angular_dist[i] = angle\n",
    "        except ValueError:\n",
    "            angular_dist[i] = 0\n",
    "            pass\n",
    "    return angular_dist\n",
    "class CMIFeDataset(Dataset):\n",
    "    def __init__(self, data_path, config):\n",
    "        self.config = config\n",
    "        self.init_feature_names(data_path)\n",
    "        df = self.generate_features(pd.read_csv(data_path, usecols=set(self.base_cols+self.feature_cols)))\n",
    "        self.generate_dataset(df)\n",
    "    def init_feature_names(self, data_path):\n",
    "        self.imu_engineered_features = ['acc_mag', 'rot_angle', 'acc_mag_jerk', 'rot_angle_vel', 'linear_acc_mag', 'linear_acc_mag_jerk', 'angular_vel_x', 'angular_vel_y', 'angular_vel_z', 'angular_distance']\n",
    "        self.tof_mode = self.config.get(\"tof_mode\", \"stats\")\n",
    "        self.tof_region_stats = ['mean', 'std', 'min', 'max']\n",
    "        self.tof_cols = self.generate_tof_feature_names()\n",
    "        columns = pd.read_csv(data_path, nrows=0).columns.tolist()\n",
    "        imu_cols_base = ['linear_acc_x', 'linear_acc_y', 'linear_acc_z']\n",
    "        imu_cols_base.extend([c for c in columns if c.startswith('rot_') and c not in ['rot_angle', 'rot_angle_vel']])\n",
    "        self.imu_cols = list(dict.fromkeys(imu_cols_base + self.imu_engineered_features))\n",
    "        self.thm_cols = [c for c in columns if c.startswith('thm_')]\n",
    "        self.feature_cols = self.imu_cols + self.thm_cols + self.tof_cols\n",
    "        self.imu_dim = len(self.imu_cols)\n",
    "        self.thm_dim = len(self.thm_cols)\n",
    "        self.tof_dim = len(self.tof_cols)\n",
    "        self.base_cols = ['acc_x', 'acc_y', 'acc_z', 'rot_x', 'rot_y', 'rot_z', 'rot_w', 'sequence_id', 'subject', 'sequence_type', 'gesture', 'orientation'] + [c for c in columns if c.startswith('thm_')] + [f\"tof_{i}_v{p}\" for i in range(1, 6) for p in range(64)]\n",
    "        self.fold_cols = ['subject', 'sequence_type', 'gesture', 'orientation']\n",
    "    def generate_tof_feature_names(self):\n",
    "        features = []\n",
    "        if self.config.get(\"tof_raw\", False):\n",
    "            for i in range(1, 6):\n",
    "                features.extend([f\"tof_{i}_v{p}\" for p in range(64)])\n",
    "        for i in range(1, 6):\n",
    "            if self.tof_mode != 0:\n",
    "                for stat in self.tof_region_stats:\n",
    "                    features.append(f'tof_{i}_{stat}')\n",
    "                if self.tof_mode > 1:\n",
    "                    for r in range(self.tof_mode):\n",
    "                        for stat in self.tof_region_stats:\n",
    "                            features.append(f'tof{self.tof_mode}_{i}_region_{r}_{stat}')\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        for r in range(mode):\n",
    "                            for stat in self.tof_region_stats:\n",
    "                                features.append(f'tof{mode}_{i}_region_{r}_{stat}')\n",
    "        return features\n",
    "    def compute_features(self, df):\n",
    "        df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n",
    "        df['rot_angle'] = 2 * np.arccos(df['rot_w'].clip(-1, 1))\n",
    "        df['acc_mag_jerk'] = df.groupby('sequence_id')['acc_mag'].diff().fillna(0)\n",
    "        df['rot_angle_vel'] = df.groupby('sequence_id')['rot_angle'].diff().fillna(0)\n",
    "        linear_accel_list = []\n",
    "        for _, group in df.groupby('sequence_id'):\n",
    "            acc_data_group = group[['acc_x', 'acc_y', 'acc_z']]\n",
    "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "            linear_accel_group = remove_gravity_from_acc(acc_data_group, rot_data_group)\n",
    "            linear_accel_list.append(pd.DataFrame(linear_accel_group, columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'], index=group.index))\n",
    "        df_linear_accel = pd.concat(linear_accel_list)\n",
    "        df = pd.concat([df, df_linear_accel], axis=1)\n",
    "        df['linear_acc_mag'] = np.sqrt(df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2)\n",
    "        df['linear_acc_mag_jerk'] = df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0)\n",
    "        angular_vel_list = []\n",
    "        for _, group in df.groupby('sequence_id'):\n",
    "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "            angular_vel_group = calculate_angular_velocity_from_quat(rot_data_group)\n",
    "            angular_vel_list.append(pd.DataFrame(angular_vel_group, columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=group.index))\n",
    "        df_angular_vel = pd.concat(angular_vel_list)\n",
    "        df = pd.concat([df, df_angular_vel], axis=1)\n",
    "        angular_distance_list = []\n",
    "        for _, group in df.groupby('sequence_id'):\n",
    "            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "            angular_dist_group = calculate_angular_distance(rot_data_group)\n",
    "            angular_distance_list.append(pd.DataFrame(angular_dist_group, columns=['angular_distance'], index=group.index))\n",
    "        df_angular_distance = pd.concat(angular_distance_list)\n",
    "        df = pd.concat([df, df_angular_distance], axis=1)\n",
    "        if self.tof_mode != 0:\n",
    "            new_columns = {}\n",
    "            for i in range(1, 6):\n",
    "                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "                tof_data = df[pixel_cols].replace(-1, np.nan)\n",
    "                new_columns.update({f'tof_{i}_mean': tof_data.mean(axis=1), f'tof_{i}_std': tof_data.std(axis=1), f'tof_{i}_min': tof_data.min(axis=1), f'tof_{i}_max': tof_data.max(axis=1)})\n",
    "                if self.tof_mode > 1:\n",
    "                    region_size = 64 // self.tof_mode\n",
    "                    for r in range(self.tof_mode):\n",
    "                        region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                        new_columns.update({f'tof{self.tof_mode}_{i}_region_{r}_mean': region_data.mean(axis=1), f'tof{self.tof_mode}_{i}_region_{r}_std': region_data.std(axis=1), f'tof{self.tof_mode}_{i}_region_{r}_min': region_data.min(axis=1), f'tof{self.tof_mode}_{i}_region_{r}_max': region_data.max(axis=1)})\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        region_size = 64 // mode\n",
    "                        for r in range(mode):\n",
    "                            region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                            new_columns.update({f'tof{mode}_{i}_region_{r}_mean': region_data.mean(axis=1), f'tof{mode}_{i}_region_{r}_std': region_data.std(axis=1), f'tof{mode}_{i}_region_{r}_min': region_data.min(axis=1), f'tof{mode}_{i}_region_{r}_max': region_data.max(axis=1)})\n",
    "            df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n",
    "        return df\n",
    "    def generate_features(self, df):\n",
    "        self.le = LabelEncoder()\n",
    "        df['gesture_int'] = self.le.fit_transform(df['gesture'])\n",
    "        self.class_num = len(self.le.classes_)\n",
    "        if all(c in df.columns for c in self.imu_engineered_features) and all(c in df.columns for c in self.tof_cols):\n",
    "            print(\"Have precomputed, skip compute.\")\n",
    "        else:\n",
    "            print(\"Not precomputed, do compute.\")\n",
    "            df = self.compute_features(df)\n",
    "        if self.config.get(\"save_precompute\", False):\n",
    "            df.to_csv(self.config.get(\"save_filename\", \"train.csv\"))\n",
    "        return df\n",
    "    def scale(self, data_unscaled):\n",
    "        scaler_function = self.config.get(\"scaler_function\", StandardScaler())\n",
    "        scaler = scaler_function.fit(np.concatenate(data_unscaled, axis=0))\n",
    "        return [scaler.transform(x) for x in data_unscaled], scaler\n",
    "    def pad(self, data_scaled, cols):\n",
    "        pad_data = np.zeros((len(data_scaled), self.pad_len, len(cols)), dtype='float32')\n",
    "        for i, seq in enumerate(data_scaled):\n",
    "            seq_len = min(len(seq), self.pad_len)\n",
    "            pad_data[i, :seq_len] = seq[:seq_len]\n",
    "        return pad_data\n",
    "    def get_nan_value(self, data, ratio):\n",
    "        max_value = data.max().max()\n",
    "        nan_value = -max_value * ratio\n",
    "        return nan_value\n",
    "    def generate_dataset(self, df):\n",
    "        seq_gp = df.groupby('sequence_id')\n",
    "        imu_unscaled, thm_unscaled, tof_unscaled = [], [], []\n",
    "        classes, lens = [], []\n",
    "        self.imu_nan_value = self.get_nan_value(df[self.imu_cols], self.config[\"nan_ratio\"][\"imu\"])\n",
    "        self.thm_nan_value = self.get_nan_value(df[self.thm_cols], self.config[\"nan_ratio\"][\"thm\"])\n",
    "        self.tof_nan_value = self.get_nan_value(df[self.tof_cols], self.config[\"nan_ratio\"][\"tof\"])\n",
    "        self.fold_feats = defaultdict(list)\n",
    "        for seq_id, seq_df in seq_gp:\n",
    "            imu_data = seq_df[self.imu_cols]\n",
    "            if self.config[\"fbfill\"][\"imu\"]:\n",
    "                imu_data = imu_data.ffill().bfill()\n",
    "            imu_unscaled.append(imu_data.fillna(self.imu_nan_value).values.astype('float32'))\n",
    "            thm_data = seq_df[self.thm_cols]\n",
    "            if self.config[\"fbfill\"][\"thm\"]:\n",
    "                thm_data = thm_data.ffill().bfill()\n",
    "            thm_unscaled.append(thm_data.fillna(self.thm_nan_value).values.astype('float32'))\n",
    "            tof_data = seq_df[self.tof_cols]\n",
    "            if self.config[\"fbfill\"][\"tof\"]:\n",
    "                tof_data = tof_data.ffill().bfill()\n",
    "            tof_unscaled.append(tof_data.fillna(self.tof_nan_value).values.astype('float32'))\n",
    "            classes.append(seq_df['gesture_int'].iloc[0])\n",
    "            lens.append(len(imu_data))\n",
    "            for col in self.fold_cols:\n",
    "                self.fold_feats[col].append(seq_df[col].iloc[0])\n",
    "        self.dataset_indices = classes\n",
    "        self.pad_len = int(np.percentile(lens, self.config.get(\"percent\", 95)))\n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            x_unscaled = [np.concatenate([imu, thm, tof], axis=1) for imu, thm, tof in zip(imu_unscaled, thm_unscaled, tof_unscaled)]\n",
    "            x_scaled, self.x_scaler = self.scale(x_unscaled)\n",
    "            x = self.pad(x_scaled, self.imu_cols+self.thm_cols+self.tof_cols)\n",
    "            self.imu = x[..., :self.imu_dim]\n",
    "            self.thm = x[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
    "            self.tof = x[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
    "        else:\n",
    "            imu_scaled, self.imu_scaler = self.scale(imu_unscaled)\n",
    "            thm_scaled, self.thm_scaler = self.scale(thm_unscaled)\n",
    "            tof_scaled, self.tof_scaler = self.scale(tof_unscaled)\n",
    "            self.imu = self.pad(imu_scaled, self.imu_cols)\n",
    "            self.thm = self.pad(thm_scaled, self.thm_cols)\n",
    "            self.tof = self.pad(tof_scaled, self.tof_cols)\n",
    "        self.precompute_scaled_nan_values()\n",
    "        self.class_ = F.one_hot(torch.from_numpy(np.array(classes)).long(), num_classes=len(self.le.classes_)).float().numpy()\n",
    "        self.class_weight = torch.FloatTensor(compute_class_weight('balanced', classes=np.arange(len(self.le.classes_)), y=classes))\n",
    "    def precompute_scaled_nan_values(self):\n",
    "        dummy_df = pd.DataFrame(np.array([[self.imu_nan_value]*len(self.imu_cols) + [self.thm_nan_value]*len(self.thm_cols) + [self.tof_nan_value]*len(self.tof_cols)]), columns=self.imu_cols + self.thm_cols + self.tof_cols)\n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            scaled = self.x_scaler.transform(dummy_df)\n",
    "            self.imu_scaled_nan = scaled[0, :self.imu_dim].mean()\n",
    "            self.thm_scaled_nan = scaled[0, self.imu_dim:self.imu_dim+self.thm_dim].mean()\n",
    "            self.tof_scaled_nan = scaled[0, self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim].mean()\n",
    "        else:\n",
    "            self.imu_scaled_nan = self.imu_scaler.transform(dummy_df[self.imu_cols])[0].mean()\n",
    "            self.thm_scaled_nan = self.thm_scaler.transform(dummy_df[self.thm_cols])[0].mean()\n",
    "            self.tof_scaled_nan = self.tof_scaler.transform(dummy_df[self.tof_cols])[0].mean()\n",
    "    def get_scaled_nan_tensors(self, imu, thm, tof):\n",
    "        return torch.full(imu.shape, self.imu_scaled_nan, device=imu.device), torch.full(thm.shape, self.thm_scaled_nan, device=thm.device), torch.full(tof.shape, self.tof_scaled_nan, device=tof.device)\n",
    "    def inference_process(self, sequence):\n",
    "        df_seq = sequence.to_pandas().copy()\n",
    "        if not all(c in df_seq.columns for c in self.imu_engineered_features):\n",
    "            df_seq['acc_mag'] = np.sqrt(df_seq['acc_x']**2 + df_seq['acc_y']**2 + df_seq['acc_z']**2)\n",
    "            df_seq['rot_angle'] = 2 * np.arccos(df_seq['rot_w'].clip(-1, 1))\n",
    "            df_seq['acc_mag_jerk'] = df_seq['acc_mag'].diff().fillna(0)\n",
    "            df_seq['rot_angle_vel'] = df_seq['rot_angle'].diff().fillna(0)\n",
    "            if all(col in df_seq.columns for col in ['acc_x', 'acc_y', 'acc_z', 'rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
    "                linear_accel = remove_gravity_from_acc(df_seq[['acc_x', 'acc_y', 'acc_z']], df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
    "                df_seq[['linear_acc_x', 'linear_acc_y', 'linear_acc_z']] = linear_accel\n",
    "            else:\n",
    "                df_seq['linear_acc_x'] = df_seq.get('acc_x', 0)\n",
    "                df_seq['linear_acc_y'] = df_seq.get('acc_y', 0)\n",
    "                df_seq['linear_acc_z'] = df_seq.get('acc_z', 0)\n",
    "            df_seq['linear_acc_mag'] = np.sqrt(df_seq['linear_acc_x']**2 + df_seq['linear_acc_y']**2 + df_seq['linear_acc_z']**2)\n",
    "            df_seq['linear_acc_mag_jerk'] = df_seq['linear_acc_mag'].diff().fillna(0)\n",
    "            if all(col in df_seq.columns for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
    "                angular_vel = calculate_angular_velocity_from_quat(df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
    "                df_seq[['angular_vel_x', 'angular_vel_y', 'angular_vel_z']] = angular_vel\n",
    "            else:\n",
    "                df_seq[['angular_vel_x', 'angular_vel_y', 'angular_vel_z']] = 0\n",
    "            if all(col in df_seq.columns for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']):\n",
    "                df_seq['angular_distance'] = calculate_angular_distance(df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
    "            else:\n",
    "                df_seq['angular_distance'] = 0\n",
    "        if self.tof_mode != 0:\n",
    "            new_columns = {}\n",
    "            for i in range(1, 6):\n",
    "                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "                tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n",
    "                new_columns.update({f'tof_{i}_mean': tof_data.mean(axis=1), f'tof_{i}_std': tof_data.std(axis=1), f'tof_{i}_min': tof_data.min(axis=1), f'tof_{i}_max': tof_data.max(axis=1)})\n",
    "                if self.tof_mode > 1:\n",
    "                    region_size = 64 // self.tof_mode\n",
    "                    for r in range(self.tof_mode):\n",
    "                        region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                        new_columns.update({f'tof{self.tof_mode}_{i}_region_{r}_mean': region_data.mean(axis=1), f'tof{self.tof_mode}_{i}_region_{r}_std': region_data.std(axis=1), f'tof{self.tof_mode}_{i}_region_{r}_min': region_data.min(axis=1), f'tof{self.tof_mode}_{i}_region_{r}_max': region_data.max(axis=1)})\n",
    "                if self.tof_mode == -1:\n",
    "                    for mode in [2, 4, 8, 16, 32]:\n",
    "                        region_size = 64 // mode\n",
    "                        for r in range(mode):\n",
    "                            region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n",
    "                            new_columns.update({f'tof{mode}_{i}_region_{r}_mean': region_data.mean(axis=1), f'tof{mode}_{i}_region_{r}_std': region_data.std(axis=1), f'tof{mode}_{i}_region_{r}_min': region_data.min(axis=1), f'tof{mode}_{i}_region_{r}_max': region_data.max(axis=1)})\n",
    "            df_seq = pd.concat([df_seq, pd.DataFrame(new_columns)], axis=1)\n",
    "        imu_unscaled = df_seq[self.imu_cols]\n",
    "        if self.config[\"fbfill\"][\"imu\"]:\n",
    "            imu_unscaled = imu_unscaled.ffill().bfill()\n",
    "        imu_unscaled = imu_unscaled.fillna(self.imu_nan_value).values.astype('float32')\n",
    "        thm_unscaled = df_seq[self.thm_cols]\n",
    "        if self.config[\"fbfill\"][\"thm\"]:\n",
    "            thm_unscaled = thm_unscaled.ffill().bfill()\n",
    "        thm_unscaled = thm_unscaled.fillna(self.thm_nan_value).values.astype('float32')\n",
    "        tof_unscaled = df_seq[self.tof_cols]\n",
    "        if self.config[\"fbfill\"][\"tof\"]:\n",
    "            tof_unscaled = tof_unscaled.ffill().bfill()\n",
    "        tof_unscaled = tof_unscaled.fillna(self.tof_nan_value).values.astype('float32')\n",
    "        if self.config.get(\"one_scale\", True):\n",
    "            x_unscaled = np.concatenate([imu_unscaled, thm_unscaled, tof_unscaled], axis=1)\n",
    "            x_scaled = self.x_scaler.transform(x_unscaled)\n",
    "            imu_scaled = x_scaled[..., :self.imu_dim]\n",
    "            thm_scaled = x_scaled[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
    "            tof_scaled = x_scaled[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
    "        else:\n",
    "            imu_scaled = self.imu_scaler.transform(imu_unscaled)\n",
    "            thm_scaled = self.thm_scaler.transform(thm_unscaled)\n",
    "            tof_scaled = self.tof_scaler.transform(tof_unscaled)\n",
    "        combined = np.concatenate([imu_scaled, thm_scaled, tof_scaled], axis=1)\n",
    "        padded = np.zeros((self.pad_len, combined.shape[1]), dtype='float32')\n",
    "        seq_len = min(combined.shape[0], self.pad_len)\n",
    "        padded[:seq_len] = combined[:seq_len]\n",
    "        imu = padded[..., :self.imu_dim]\n",
    "        thm = padded[..., self.imu_dim:self.imu_dim+self.thm_dim]\n",
    "        tof = padded[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n",
    "        return torch.from_numpy(imu).float().unsqueeze(0), torch.from_numpy(thm).float().unsqueeze(0), torch.from_numpy(tof).float().unsqueeze(0)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.imu[idx], self.thm[idx], self.tof[idx], self.class_[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.class_)\n",
    "class CMIFoldDataset:\n",
    "    def __init__(self, data_path, config, full_dataset_function, n_folds=5, random_seed=0):\n",
    "        self.full_dataset = full_dataset_function(data_path=data_path, config=config)\n",
    "        self.imu_dim = self.full_dataset.imu_dim\n",
    "        self.thm_dim = self.full_dataset.thm_dim\n",
    "        self.tof_dim = self.full_dataset.tof_dim\n",
    "        self.le = self.full_dataset.le\n",
    "        self.class_names = self.full_dataset.le.classes_\n",
    "        self.class_weight = self.full_dataset.class_weight\n",
    "        self.n_folds = n_folds\n",
    "        self.skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n",
    "        self.folds = list(self.skf.split(np.arange(len(self.full_dataset)), np.array(self.full_dataset.dataset_indices)))\n",
    "    def get_fold_datasets(self, fold_idx):\n",
    "        if self.folds is None or fold_idx >= self.n_folds:\n",
    "            return None, None\n",
    "        fold_train_idx, fold_valid_idx = self.folds[fold_idx]\n",
    "        return Subset(self.full_dataset, fold_train_idx), Subset(self.full_dataset, fold_valid_idx)\n",
    "    def print_fold_stats(self):\n",
    "        def get_label_counts(subset):\n",
    "            counts = {name: 0 for name in self.class_names}\n",
    "            if subset is None:\n",
    "                return counts\n",
    "            for idx in subset.indices:\n",
    "                label_idx = self.full_dataset.dataset_indices[idx]\n",
    "                counts[self.class_names[label_idx]] += 1\n",
    "            return counts\n",
    "        print(\"\\nCross-validation fold statistics:\")\n",
    "        for fold_idx in range(self.n_folds):\n",
    "            train_fold, valid_fold = self.get_fold_datasets(fold_idx)\n",
    "            train_counts = get_label_counts(train_fold)\n",
    "            valid_counts = get_label_counts(valid_fold)\n",
    "            print(f\"\\nFold {fold_idx + 1}:\")\n",
    "            print(f\"{'Category':<50} {'Training Set':<10} {'Validation Set':<10}\")\n",
    "            for name in self.class_names:\n",
    "                print(f\"{name:<50} {train_counts[name]:<10} {valid_counts[name]:<10}\")\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=8):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction, bias=True)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        se = F.adaptive_avg_pool1d(x, 1).squeeze(-1)\n",
    "        se = F.relu(self.fc1(se), inplace=True)\n",
    "        se = self.sigmoid(self.fc2(se)).unsqueeze(-1)\n",
    "        return x * se\n",
    "class ResNetSEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, wd=1e-4):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.se = SEBlock(out_channels)\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(nn.Conv1d(in_channels, out_channels, kernel_size=1, padding=0, bias=False), nn.BatchNorm1d(out_channels))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out)\n",
    "        out = out + identity\n",
    "        return self.relu(out)\n",
    "class CMIModel(nn.Module):\n",
    "    def __init__(self, imu_dim, thm_dim, tof_dim, n_classes, **kwargs):\n",
    "        super().__init__()\n",
    "        self.imu_branch = nn.Sequential(self.residual_se_cnn_block(imu_dim, kwargs[\"imu1_channels\"], kwargs[\"imu1_layers\"], drop=kwargs[\"imu1_dropout\"]), self.residual_se_cnn_block(kwargs[\"imu1_channels\"], kwargs[\"feat_dim\"], kwargs[\"imu2_layers\"], drop=kwargs[\"imu2_dropout\"]))\n",
    "        self.thm_branch = nn.Sequential(nn.Conv1d(thm_dim, kwargs[\"thm1_channels\"], kernel_size=3, padding=1, bias=False), nn.BatchNorm1d(kwargs[\"thm1_channels\"]), nn.ReLU(inplace=True), nn.MaxPool1d(2, ceil_mode=True), nn.Dropout(kwargs[\"thm1_dropout\"]), nn.Conv1d(kwargs[\"thm1_channels\"], kwargs[\"feat_dim\"], kernel_size=3, padding=1, bias=False), nn.BatchNorm1d(kwargs[\"feat_dim\"]), nn.ReLU(inplace=True), nn.MaxPool1d(2, ceil_mode=True), nn.Dropout(kwargs[\"thm2_dropout\"]))\n",
    "        self.tof_branch = nn.Sequential(nn.Conv1d(tof_dim, kwargs[\"tof1_channels\"], kernel_size=3, padding=1, bias=False), nn.BatchNorm1d(kwargs[\"tof1_channels\"]), nn.ReLU(inplace=True), nn.MaxPool1d(2, ceil_mode=True), nn.Dropout(kwargs[\"tof1_dropout\"]), nn.Conv1d(kwargs[\"tof1_channels\"], kwargs[\"feat_dim\"], kernel_size=3, padding=1, bias=False), nn.BatchNorm1d(kwargs[\"feat_dim\"]), nn.ReLU(inplace=True), nn.MaxPool1d(2, ceil_mode=True), nn.Dropout(kwargs[\"tof2_dropout\"]))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, kwargs[\"feat_dim\"]))\n",
    "        self.bert = BertModel(BertConfig(hidden_size=kwargs[\"feat_dim\"], num_hidden_layers=kwargs[\"bert_layers\"], num_attention_heads=kwargs[\"bert_heads\"], intermediate_size=kwargs[\"feat_dim\"]*4))\n",
    "        self.classifier = nn.Sequential(nn.Linear(kwargs[\"feat_dim\"], kwargs[\"cls1_channels\"], bias=False), nn.BatchNorm1d(kwargs[\"cls1_channels\"]), nn.ReLU(inplace=True), nn.Dropout(kwargs[\"cls1_dropout\"]), nn.Linear(kwargs[\"cls1_channels\"], kwargs[\"cls2_channels\"], bias=False), nn.BatchNorm1d(kwargs[\"cls2_channels\"]), nn.ReLU(inplace=True), nn.Dropout(kwargs[\"cls2_dropout\"]), nn.Linear(kwargs[\"cls2_channels\"], n_classes))\n",
    "    def residual_se_cnn_block(self, in_channels, out_channels, num_layers, pool_size=2, drop=0.3, wd=1e-4):\n",
    "        return nn.Sequential(*[ResNetSEBlock(in_channels=in_channels, out_channels=in_channels) for i in range(num_layers)], ResNetSEBlock(in_channels, out_channels, wd=wd), nn.MaxPool1d(pool_size), nn.Dropout(drop))\n",
    "    def forward(self, imu, thm, tof):\n",
    "        imu_feat = self.imu_branch(imu.permute(0, 2, 1))\n",
    "        thm_feat = self.thm_branch(thm.permute(0, 2, 1))\n",
    "        tof_feat = self.tof_branch(tof.permute(0, 2, 1))\n",
    "        bert_input = torch.cat([imu_feat, thm_feat, tof_feat], dim=-1).permute(0, 2, 1)\n",
    "        cls_token = self.cls_token.expand(bert_input.size(0), -1, -1)\n",
    "        bert_input = torch.cat([cls_token, bert_input], dim=1)\n",
    "        outputs = self.bert(inputs_embeds=bert_input)\n",
    "        pred_cls = outputs.last_hidden_state[:, 0, :]\n",
    "        return self.classifier(pred_cls)\n",
    "CUDA0 = \"cuda:0\"\n",
    "seed = 0\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "n_folds = 5\n",
    "universe_csv_path = Path(\"/kaggle/input/cmi-precompute/pytorch/all/1/tof-1_raw.csv\")\n",
    "deterministic = kagglehub.package_import('wasupandceacar/deterministic').deterministic\n",
    "deterministic.init_all(seed)\n",
    "def init_dataset():\n",
    "    dataset_config = {\"percent\": 95, \"scaler_function\": StandardScaler(), \"nan_ratio\": {\"imu\": 0, \"thm\": 0, \"tof\": 0}, \"fbfill\": {\"imu\": True, \"thm\": True, \"tof\": True}, \"one_scale\": True, \"tof_raw\": True, \"tof_mode\": 16, \"save_precompute\": False}\n",
    "    dataset = CMIFoldDataset(universe_csv_path, dataset_config, n_folds=n_folds, random_seed=seed, full_dataset_function=CMIFeDataset)\n",
    "    dataset.print_fold_stats()\n",
    "    return dataset\n",
    "def get_fold_dataset(dataset, fold):\n",
    "    _, valid_dataset = dataset.get_fold_datasets(fold)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "    return valid_loader\n",
    "dataset = init_dataset()\n",
    "model_function = CMIModel\n",
    "model_args = {\"feat_dim\": 500, \"imu1_channels\": 219, \"imu1_dropout\": 0.2946731587132302, \"imu2_dropout\": 0.2697745571929592, \"imu1_weight_decay\": 0.0014824054650601245, \"imu2_weight_decay\": 0.002742543773142381, \"imu1_layers\": 0, \"imu2_layers\": 0, \"thm1_channels\": 82, \"thm1_dropout\": 0.2641274454844602, \"thm2_dropout\": 0.302896343020985, \"tof1_channels\": 82, \"tof1_dropout\": 0.2641274454844602, \"tof2_dropout\": 0.3028963430209852, \"bert_layers\": 8, \"bert_heads\": 10, \"cls1_channels\": 937, \"cls2_channels\": 303, \"cls1_dropout\": 0.2281834512100508, \"cls2_dropout\": 0.22502521933558461}\n",
    "model_args.update({\"imu_dim\": dataset.full_dataset.imu_dim, \"thm_dim\": dataset.full_dataset.thm_dim, \"tof_dim\": dataset.full_dataset.tof_dim, \"n_classes\": dataset.full_dataset.class_num})\n",
    "model_dir = Path(\"/kaggle/input/cmi-models-public/pytorch/train_fold_model05_tof16_raw/1\")\n",
    "model_dicts = [{\"model_function\": model_function, \"model_args\": model_args, \"model_path\": model_dir / f\"fold{fold}/best_ema.pt\"} for fold in range(n_folds)]\n",
    "models2 = list()\n",
    "for model_dict in model_dicts:\n",
    "    model_function = model_dict[\"model_function\"]\n",
    "    model_args = model_dict[\"model_args\"]\n",
    "    model_path = model_dict[\"model_path\"]\n",
    "    model = model_function(**model_args).to(CUDA0)\n",
    "    state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in torch.load(model_path).items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.eval()\n",
    "    models2.append(model)\n",
    "metric_package = kagglehub.package_import('wasupandceacar/cmi-metric')\n",
    "metric = metric_package.Metric()\n",
    "imu_only_metric = metric_package.Metric()\n",
    "def to_cuda(*tensors):\n",
    "    return [tensor.to(CUDA0) for tensor in tensors]\n",
    "def predict_valid(model, imu, thm, tof):\n",
    "    pred = model(imu, thm, tof)\n",
    "    return pred\n",
    "def valid(model, valid_bar):\n",
    "    with torch.no_grad():\n",
    "        for imu, thm, tof, y in valid_bar:\n",
    "            imu, thm, tof, y = to_cuda(imu, thm, tof, y)\n",
    "            with autocast(device_type='cuda', dtype=torch.bfloat16): \n",
    "                logits = predict_valid(model, imu, thm, tof)\n",
    "            metric.add(dataset.le.classes_[y.argmax(dim=1).cpu()], dataset.le.classes_[logits.argmax(dim=1).cpu()])\n",
    "            _, thm, tof = dataset.full_dataset.get_scaled_nan_tensors(imu, thm, tof)\n",
    "            with autocast(device_type='cuda', dtype=torch.bfloat16): \n",
    "                logits = model(imu, thm, tof)\n",
    "            imu_only_metric.add(dataset.le.classes_[y.argmax(dim=1).cpu()], dataset.le.classes_[logits.argmax(dim=1).cpu()])\n",
    "def avg_predict(models, imu, thm, tof):\n",
    "    outputs = []\n",
    "    with autocast(device_type='cuda'):\n",
    "        for model in models:\n",
    "            logits = model(imu, thm, tof)\n",
    "        outputs.append(logits)\n",
    "    return torch.mean(torch.stack(outputs), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4750565",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:51:02.812924Z",
     "iopub.status.busy": "2025-08-13T05:51:02.812403Z",
     "iopub.status.idle": "2025-08-13T05:51:02.816711Z",
     "shell.execute_reply": "2025-08-13T05:51:02.816130Z"
    },
    "papermill": {
     "duration": 0.01086,
     "end_time": "2025-08-13T05:51:02.817966",
     "exception": false,
     "start_time": "2025-08-13T05:51:02.807106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_2\n",
    "def predict2(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    imu, thm, tof = dataset.full_dataset.inference_process(sequence)\n",
    "    with torch.no_grad():\n",
    "        imu, thm, tof = to_cuda(imu, thm, tof)\n",
    "        logits = avg_predict(models2, imu, thm, tof)\n",
    "        probabilities = F.softmax(logits, dim=1).cpu().numpy()\n",
    "    return probabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20759106",
   "metadata": {
    "papermill": {
     "duration": 0.005579,
     "end_time": "2025-08-13T05:51:02.828382",
     "exception": false,
     "start_time": "2025-08-13T05:51:02.822803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42de2b1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:51:02.839547Z",
     "iopub.status.busy": "2025-08-13T05:51:02.839296Z",
     "iopub.status.idle": "2025-08-13T05:51:08.984282Z",
     "shell.execute_reply": "2025-08-13T05:51:08.983514Z"
    },
    "papermill": {
     "duration": 6.152219,
     "end_time": "2025-08-13T05:51:08.985496",
     "exception": false,
     "start_time": "2025-08-13T05:51:02.833277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompetitionMetric could not be imported. OOF/CV score will not be calculated.\n",
      "▶ Library import complete\n",
      " - TensorFlow: 2.18.0\n",
      " - PyTorch: 2.6.0+cu124\n",
      "▶ TRAIN mode: False\n",
      "▶ Inference mode start – loading trained models and artifacts...\n",
      " Loading model group A (Custom 5-Fold Gated GRU model)...\n",
      " > Loaded 10 models successfully.\n",
      "\n",
      " Loading model group B (Public TF/Keras model)...\n",
      " > Loaded 1 model successfully.\n",
      "\n",
      " Loading model group C (Public PyTorch model)...\n",
      " > Loaded 5 models successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, BatchNormalization, Activation, add, MaxPooling1D, Dropout,\n",
    "    Bidirectional, GRU, GlobalAveragePooling1D, Dense, Multiply, Reshape,\n",
    "    Lambda, Concatenate\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam as AdamTF\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import Sequence, to_categorical, pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam as AdamTorch\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from scipy.signal import firwin\n",
    "try:\n",
    "    from cmi_2025_metric_copy_for_import import CompetitionMetric\n",
    "except ImportError:\n",
    "    CompetitionMetric = None\n",
    "    print(\"CompetitionMetric could not be imported. OOF/CV score will not be calculated.\")\n",
    "def seed_everything(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(2025)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.experimental.numpy.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "seed_everything(seed=42)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "TRAIN = False\n",
    "RAW_DIR = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\n",
    "YOUR_MODELS_DIR = Path(\"/kaggle/input/cmi-data-gated-gru\")\n",
    "PUBLIC_TF_MODEL_DIR = Path(\"/kaggle/input/lb-0-78-quaternions-tf-bilstm-gru-attention\")\n",
    "PUBLIC_PT_MODEL_DIR = Path(\"/kaggle/input/cmi3-models-p\")\n",
    "EXPORT_DIR = Path(\"./\")\n",
    "BATCH_SIZE = 64\n",
    "PAD_PERCENTILE = 95\n",
    "LR_INIT = 4e-4\n",
    "WD = 3e-3\n",
    "MIXUP_ALPHA = 0.4\n",
    "EPOCHS = 360\n",
    "PATIENCE = 50\n",
    "N_SPLITS = 10\n",
    "MASKING_PROB = 0.25\n",
    "GATE_LOSS_WEIGHT = 0.2\n",
    "print(f\"▶ Library import complete\")\n",
    "print(f\" - TensorFlow: {tf.__version__}\")\n",
    "print(f\" - PyTorch: {torch.__version__}\")\n",
    "print(f\"▶ TRAIN mode: {TRAIN}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mean_pt = torch.tensor([0, 0, 0, 0, 0, 0, 9.0319e-03, 1.0849e+00, -2.6186e-03, 3.7651e-03, -5.3660e-03, -2.8177e-03, 1.3318e-03, -1.5876e-04, 6.3495e-01, 6.2877e-01, 6.0607e-01, 6.2142e-01, 6.3808e-01, 6.5420e-01, 7.4102e-03, -3.4159e-03, -7.5237e-03, -2.6034e-02, 2.9704e-02, -3.1546e-02, -2.0610e-03, -4.6986e-03, -4.7216e-03, -2.6281e-02, 1.5799e-02, 1.0016e-02], dtype=torch.float32).view(1, -1, 1).to(device)\n",
    "std_pt = torch.tensor([1, 1, 1, 1, 1, 1, 0.2067, 0.8583, 0.3162, 0.2668, 0.2917, 0.2341, 0.3023, 0.3281, 1.0264, 0.8838, 0.8686, 1.0973, 1.0267, 0.9018, 0.4658, 0.2009, 0.2057, 1.2240, 0.9535, 0.6655, 0.2941, 0.3421, 0.8156, 0.6565, 1.1034, 1.5577], dtype=torch.float32).view(1, -1, 1).to(device) + 1e-8\n",
    "class ImuFeatureExtractor(nn.Module):\n",
    "    def __init__(self, fs=100., add_quaternion=False):\n",
    "        super().__init__()\n",
    "        self.fs = fs\n",
    "        self.add_quaternion = add_quaternion\n",
    "        k = 15\n",
    "        self.lpf = nn.Conv1d(6, 6, kernel_size=k, padding=k//2, groups=6, bias=False)\n",
    "        nn.init.kaiming_uniform_(self.lpf.weight, a=math.sqrt(5))\n",
    "        self.lpf_acc = nn.Conv1d(3, 3, k, padding=k//2, groups=3, bias=False)\n",
    "        self.lpf_gyro = nn.Conv1d(3, 3, k, padding=k//2, groups=3, bias=False)\n",
    "    def forward(self, imu):\n",
    "        acc = imu[:, 0:3, :]\n",
    "        gyro = imu[:, 3:6, :]\n",
    "        acc_mag = torch.norm(acc, dim=1, keepdim=True)\n",
    "        gyro_mag = torch.norm(gyro, dim=1, keepdim=True)\n",
    "        jerk = F.pad(acc[:, :, 1:] - acc[:, :, :-1], (1,0))\n",
    "        gyro_delta = F.pad(gyro[:, :, 1:] - gyro[:, :, :-1], (1,0))\n",
    "        acc_pow = acc ** 2\n",
    "        gyro_pow = gyro ** 2\n",
    "        acc_lpf = self.lpf_acc(acc)\n",
    "        acc_hpf = acc - acc_lpf\n",
    "        gyro_lpf = self.lpf_gyro(gyro)\n",
    "        gyro_hpf = gyro - gyro_lpf\n",
    "        features = [acc, gyro, acc_mag, gyro_mag, jerk, gyro_delta, acc_pow, gyro_pow, acc_lpf, acc_hpf, gyro_lpf, gyro_hpf]\n",
    "        return torch.cat(features, dim=1)\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=8):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool1d(1)\n",
    "        self.excitation = nn.Sequential(nn.Linear(channels, channels // reduction, bias=False), nn.ReLU(inplace=True), nn.Linear(channels // reduction, channels, bias=False), nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.squeeze(x).view(b, c)\n",
    "        y = self.excitation(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "class ResidualSECNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, pool_size=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.se = SEBlock(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(nn.Conv1d(in_channels, out_channels, 1, bias=False), nn.BatchNorm1d(out_channels))\n",
    "        self.pool = nn.MaxPool1d(pool_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out)\n",
    "        out += self.shortcut(x)\n",
    "        return self.dropout(self.pool(F.relu(out)))\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "    def forward(self, x):\n",
    "        scores = torch.tanh(self.attention(x))\n",
    "        weights = F.softmax(scores.squeeze(-1), dim=1)\n",
    "        return torch.sum(x * weights.unsqueeze(-1), dim=1)\n",
    "class TwoBranchModel(nn.Module):\n",
    "    def __init__(self, pad_len, imu_dim_raw, tof_dim, n_classes, dropouts=[0.3, 0.3, 0.3, 0.3, 0.4, 0.5, 0.3], feature_engineering=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.feature_engineering = feature_engineering\n",
    "        imu_dim = 32 if feature_engineering else imu_dim_raw\n",
    "        self.imu_fe = ImuFeatureExtractor(**kwargs) if feature_engineering else nn.Identity()\n",
    "        self.fir_nchan = 7\n",
    "        numtaps = 33\n",
    "        fir_kernel = torch.tensor(firwin(numtaps, cutoff=1.0, fs=10.0, pass_zero=False), dtype=torch.float32).view(1, 1, -1).repeat(self.fir_nchan, 1, 1)\n",
    "        self.register_buffer(\"fir_kernel\", fir_kernel)\n",
    "        self.imu_block1 = ResidualSECNNBlock(imu_dim, 64, 3, dropout=dropouts[0])\n",
    "        self.imu_block2 = ResidualSECNNBlock(64, 128, 5, dropout=dropouts[1])\n",
    "        self.tof_conv1 = nn.Conv1d(tof_dim, 64, 3, padding=1, bias=False)\n",
    "        self.tof_bn1, self.tof_pool1, self.tof_drop1 = nn.BatchNorm1d(64), nn.MaxPool1d(2), nn.Dropout(dropouts[2])\n",
    "        self.tof_conv2 = nn.Conv1d(64, 128, 3, padding=1, bias=False)\n",
    "        self.tof_bn2, self.tof_pool2, self.tof_drop2 = nn.BatchNorm1d(128), nn.MaxPool1d(2), nn.Dropout(dropouts[3])\n",
    "        self.bilstm = nn.LSTM(256, 128, bidirectional=True, batch_first=True)\n",
    "        self.lstm_dropout = nn.Dropout(dropouts[4])\n",
    "        self.attention = AttentionLayer(256)\n",
    "        self.dense1, self.bn_dense1, self.drop1 = nn.Linear(256, 256, bias=False), nn.BatchNorm1d(256), nn.Dropout(dropouts[5])\n",
    "        self.dense2, self.bn_dense2, self.drop2 = nn.Linear(256, 128, bias=False), nn.BatchNorm1d(128), nn.Dropout(dropouts[6])\n",
    "        self.classifier = nn.Linear(128, n_classes)\n",
    "    def forward(self, x):\n",
    "        imu_raw = x[:, :, :self.fir_nchan].transpose(1, 2)\n",
    "        tof = x[:, :, self.fir_nchan:].transpose(1, 2)\n",
    "        imu_fe = self.imu_fe(imu_raw)\n",
    "        filtered = F.conv1d(imu_fe[:, :self.fir_nchan, :], self.fir_kernel, padding=self.fir_kernel.shape[-1] // 2, groups=self.fir_nchan)\n",
    "        imu = (torch.cat([filtered, imu_fe[:, self.fir_nchan:, :]], dim=1) - mean_pt) / std_pt\n",
    "        x1 = self.imu_block1(imu); x1 = self.imu_block2(x1)\n",
    "        x2 = self.tof_drop1(self.tof_pool1(F.relu(self.tof_bn1(self.tof_conv1(tof)))))\n",
    "        x2 = self.tof_drop2(self.tof_pool2(F.relu(self.tof_bn2(self.tof_conv2(x2)))))\n",
    "        merged = torch.cat([x1, x2], dim=1).transpose(1, 2)\n",
    "        lstm_out, _ = self.bilstm(merged); lstm_out = self.lstm_dropout(lstm_out)\n",
    "        attended = self.attention(lstm_out)\n",
    "        x = self.drop1(F.relu(self.bn_dense1(self.dense1(attended))))\n",
    "        x = self.drop2(F.relu(self.bn_dense2(self.dense2(x))))\n",
    "        return self.classifier(x)\n",
    "class PublicTwoBranchModel(nn.Module):\n",
    "    def __init__(self, pad_len, imu_dim_raw, tof_dim, n_classes, dropouts=[0.3, 0.3, 0.3, 0.3, 0.4, 0.5, 0.3], feature_engineering=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.feature_engineering = feature_engineering\n",
    "        imu_dim = 32 if feature_engineering else imu_dim_raw\n",
    "        self.imu_fe = ImuFeatureExtractor(**kwargs) if feature_engineering else nn.Identity()\n",
    "        self.fir_nchan = 7\n",
    "        numtaps = 33\n",
    "        fir_kernel = torch.tensor(firwin(numtaps, cutoff=1.0, fs=10.0, pass_zero=False), dtype=torch.float32).view(1, 1, -1).repeat(self.fir_nchan, 1, 1)\n",
    "        self.register_buffer(\"fir_kernel\", fir_kernel)\n",
    "        self.imu_block1 = ResidualSECNNBlock(imu_dim, 64, 3, dropout=dropouts[0])\n",
    "        self.imu_block2 = ResidualSECNNBlock(64, 128, 5, dropout=dropouts[1])\n",
    "        self.tof_conv1 = nn.Conv1d(tof_dim, 64, 3, padding=1, bias=False)\n",
    "        self.tof_bn1, self.tof_pool1, self.tof_drop1 = nn.BatchNorm1d(64), nn.MaxPool1d(2), nn.Dropout(dropouts[2])\n",
    "        self.tof_conv2 = nn.Conv1d(64, 128, 3, padding=1, bias=False)\n",
    "        self.tof_bn2, self.tof_pool2, self.tof_drop2 = nn.BatchNorm1d(128), nn.MaxPool1d(2), nn.Dropout(dropouts[3])\n",
    "        self.bilstm = nn.LSTM(256, 128, bidirectional=True, batch_first=True)\n",
    "        self.lstm_dropout = nn.Dropout(dropouts[4])\n",
    "        self.attention = AttentionLayer(256)\n",
    "        self.dense1, self.bn_dense1, self.drop1 = nn.Linear(256, 256, bias=False), nn.BatchNorm1d(256), nn.Dropout(dropouts[5])\n",
    "        self.dense2, self.bn_dense2, self.drop2 = nn.Linear(256, 128, bias=False), nn.BatchNorm1d(128), nn.Dropout(dropouts[6])\n",
    "        self.classifier = nn.Linear(128, n_classes)\n",
    "    def forward(self, x):\n",
    "        imu_raw = x[:, :, :self.fir_nchan].transpose(1, 2)\n",
    "        tof = x[:, :, self.fir_nchan:].transpose(1, 2)\n",
    "        imu_fe = self.imu_fe(imu_raw)\n",
    "        filtered = F.conv1d(imu_fe[:, :self.fir_nchan, :], self.fir_kernel, padding=self.fir_kernel.shape[-1] // 2, groups=self.fir_nchan)\n",
    "        imu = (torch.cat([filtered, imu_fe[:, self.fir_nchan:, :]], dim=1) - mean_pt) / std_pt\n",
    "        x1 = self.imu_block1(imu); x1 = self.imu_block2(x1)\n",
    "        x2 = self.tof_drop1(self.tof_pool1(F.relu(self.tof_bn1(self.tof_conv1(tof)))))\n",
    "        x2 = self.tof_drop2(self.tof_pool2(F.relu(self.tof_bn2(self.tof_conv2(x2)))))\n",
    "        merged = torch.cat([x1, x2], dim=1).transpose(1, 2)\n",
    "        lstm_out, _ = self.bilstm(merged); lstm_out = self.lstm_dropout(lstm_out)\n",
    "        attended = self.attention(lstm_out)\n",
    "        x = self.drop1(F.relu(self.bn_dense1(self.dense1(attended))))\n",
    "        x = self.drop2(F.relu(self.bn_dense2(self.dense2(x))))\n",
    "        return self.classifier(x)\n",
    "def pad_sequences_torch3(sequences, maxlen, padding='post', truncating='post', value=0.0):\n",
    "    result = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) >= maxlen: seq = seq[:maxlen] if truncating == 'post' else seq[-maxlen:]\n",
    "        else:\n",
    "            pad_len = maxlen - len(seq)\n",
    "            pad_array = np.full((pad_len, seq.shape[1]), value)\n",
    "            seq = np.concatenate([seq, pad_array]) if padding == 'post' else np.concatenate([pad_array, seq])\n",
    "        result.append(seq)\n",
    "    return np.array(result, dtype=np.float32)\n",
    "def remove_gravity_from_acc3(acc_data, rot_data):\n",
    "    acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "    for i in range(len(acc_values)):\n",
    "        if np.all(np.isnan(quat_values[i])):\n",
    "            linear_accel[i, :] = acc_values[i, :]\n",
    "            continue\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except (ValueError, IndexError):\n",
    "            linear_accel[i, :] = acc_values[i, :]\n",
    "    return linear_accel\n",
    "def calculate_angular_velocity_from_quat3(rot_data, time_delta=1/200):\n",
    "    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    angular_vel = np.zeros((len(quat_values), 3))\n",
    "    for i in range(len(quat_values) - 1):\n",
    "        q_t, q_t_plus_dt = quat_values[i], quat_values[i+1]\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isnan(q_t_plus_dt)): continue\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except (ValueError, IndexError): pass\n",
    "    return angular_vel\n",
    "def calculate_angular_distance3(rot_data):\n",
    "    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    angular_dist = np.zeros(len(quat_values))\n",
    "    for i in range(len(quat_values) - 1):\n",
    "        q1, q2 = quat_values[i], quat_values[i+1]\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isnan(q2)): continue\n",
    "        try:\n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            angular_dist[i] = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "        except (ValueError, IndexError): pass\n",
    "    return angular_dist\n",
    "def time_sum(x): return K.sum(x, axis=1)\n",
    "def squeeze_last_axis(x): return tf.squeeze(x, axis=-1)\n",
    "def expand_last_axis(x): return tf.expand_dims(x, axis=-1)\n",
    "def se_block(x, reduction=8):\n",
    "    ch = x.shape[-1]\n",
    "    se = GlobalAveragePooling1D()(x)\n",
    "    se = Dense(ch // reduction, activation='relu')(se)\n",
    "    se = Dense(ch, activation='sigmoid')(se)\n",
    "    se = Reshape((1, ch))(se)\n",
    "    return Multiply()([x, se])\n",
    "def residual_se_cnn_block(x, filters, kernel_size, pool_size=2, drop=0.3, wd=1e-4):\n",
    "    shortcut = x\n",
    "    for _ in range(2):\n",
    "        x = Conv1D(filters, kernel_size, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "    x = se_block(x)\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, 1, padding='same', use_bias=False, kernel_regularizer=l2(wd))(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    x = add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(pool_size)(x)\n",
    "    x = Dropout(drop)(x)\n",
    "    return x\n",
    "def attention_layer(inputs):\n",
    "    score = Dense(1, activation='tanh')(inputs)\n",
    "    score = Lambda(squeeze_last_axis)(score)\n",
    "    weights = Activation('softmax')(score)\n",
    "    weights = Lambda(expand_last_axis)(weights)\n",
    "    context = Multiply()([inputs, weights])\n",
    "    context = Lambda(time_sum)(context)\n",
    "    return context\n",
    "class GatedMixupGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size, imu_dim, class_weight=None, alpha=0.2, masking_prob=0.0):\n",
    "        self.X, self.y, self.batch, self.imu_dim = X, y, batch_size, imu_dim\n",
    "        self.class_weight, self.alpha, self.masking_prob = class_weight, alpha, masking_prob\n",
    "        self.indices = np.arange(len(X))\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch))\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i*self.batch:(i+1)*self.batch]\n",
    "        Xb, yb = self.X[idx].copy(), self.y[idx].copy()\n",
    "        sample_weights = np.ones(len(Xb), dtype='float32')\n",
    "        if self.class_weight:\n",
    "            sample_weights = np.array([self.class_weight.get(i, 1.0) for i in yb.argmax(axis=1)])\n",
    "        gate_target = np.ones(len(Xb), dtype='float32')\n",
    "        if self.masking_prob > 0:\n",
    "            for j in range(len(Xb)):\n",
    "                if np.random.rand() < self.masking_prob:\n",
    "                    Xb[j, :, self.imu_dim:] = 0\n",
    "                    gate_target[j] = 0.0\n",
    "        if self.alpha > 0:\n",
    "            lam = np.random.beta(self.alpha, self.alpha)\n",
    "            perm = np.random.permutation(len(Xb))\n",
    "            X_mix = lam * Xb + (1 - lam) * Xb[perm]\n",
    "            y_mix = lam * yb + (1 - lam) * yb[perm]\n",
    "            gate_target_mix = lam * gate_target + (1 - lam) * gate_target[perm]\n",
    "            sample_weights_mix = lam * sample_weights + (1 - lam) * sample_weights[perm]\n",
    "            return X_mix, {'main_output': y_mix, 'tof_gate': gate_target_mix}, sample_weights_mix\n",
    "        return Xb, {'main_output': yb, 'tof_gate': gate_target}, sample_weights\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "def build_gated_two_branch_model(pad_len, imu_dim, tof_dim, n_classes, wd=1e-4):\n",
    "    inp = Input(shape=(pad_len, imu_dim + tof_dim))\n",
    "    imu = Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "    tof = Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "    x1 = residual_se_cnn_block(imu, 64, 3, drop=0.1, wd=wd)\n",
    "    x1 = residual_se_cnn_block(x1, 128, 5, drop=0.1, wd=wd)\n",
    "    x2_base = Conv1D(64, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(tof)\n",
    "    x2_base = BatchNormalization()(x2_base); x2_base = Activation('relu')(x2_base)\n",
    "    x2_base = MaxPooling1D(2)(x2_base); x2_base = Dropout(0.2)(x2_base)\n",
    "    x2_base = Conv1D(128, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x2_base)\n",
    "    x2_base = BatchNormalization()(x2_base); x2_base = Activation('relu')(x2_base)\n",
    "    x2_base = MaxPooling1D(2)(x2_base); x2_base = Dropout(0.2)(x2_base)\n",
    "    gate_input = GlobalAveragePooling1D()(tof)\n",
    "    gate_input = Dense(16, activation='relu')(gate_input)\n",
    "    gate = Dense(1, activation='sigmoid', name='tof_gate')(gate_input)\n",
    "    x2 = Multiply()([x2_base, gate])\n",
    "    merged = Concatenate()([x1, x2])\n",
    "    x = Bidirectional(GRU(256, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n",
    "    x = Dropout(0.45)(x)\n",
    "    x = attention_layer(x)\n",
    "    for units, drop in [(512, 0.5), (256, 0.4), (128, 0.3)]:\n",
    "        x = Dense(units, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(drop)(x)\n",
    "    out = Dense(n_classes, activation='softmax', name='main_output', kernel_regularizer=l2(wd))(x)\n",
    "    return Model(inputs=inp, outputs=[out, gate])\n",
    "print(\"▶ Inference mode start – loading trained models and artifacts...\")\n",
    "print(\" Loading model group A (Custom 5-Fold Gated GRU model)...\")\n",
    "final_feature_cols_A = np.load(YOUR_MODELS_DIR / \"final_feature_cols.npy\", allow_pickle=True).tolist()\n",
    "pad_len_A = int(np.load(YOUR_MODELS_DIR / \"sequence_maxlen.npy\"))\n",
    "scaler_A = joblib.load(YOUR_MODELS_DIR / \"scaler.pkl\")\n",
    "gesture_classes = np.load(YOUR_MODELS_DIR / \"gesture_classes.npy\", allow_pickle=True)\n",
    "custom_objs_A = {'time_sum': time_sum, 'squeeze_last_axis': squeeze_last_axis, 'expand_last_axis': expand_last_axis, 'se_block': se_block, 'residual_se_cnn_block': residual_se_cnn_block, 'attention_layer': attention_layer}\n",
    "models_A = [load_model(YOUR_MODELS_DIR / f\"final_model_fold_{f}.h5\", compile=False, custom_objects=custom_objs_A) for f in range(N_SPLITS)]\n",
    "print(f\" > Loaded {len(models_A)} models successfully.\")\n",
    "print(\"\\n Loading model group B (Public TF/Keras model)...\")\n",
    "final_feature_cols_B = np.load(PUBLIC_TF_MODEL_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "pad_len_B = int(np.load(PUBLIC_TF_MODEL_DIR / \"sequence_maxlen.npy\"))\n",
    "scaler_B = joblib.load(PUBLIC_TF_MODEL_DIR / \"scaler.pkl\")\n",
    "custom_objs_B = custom_objs_A\n",
    "model_B = load_model(PUBLIC_TF_MODEL_DIR / \"gesture_two_branch_mixup.h5\", compile=False, custom_objects=custom_objs_B)\n",
    "print(\" > Loaded 1 model successfully.\")\n",
    "print(\"\\n Loading model group C (Public PyTorch model)...\")\n",
    "final_feature_cols_C = np.load(PUBLIC_PT_MODEL_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "pad_len_C = int(np.load(PUBLIC_PT_MODEL_DIR / \"sequence_maxlen.npy\"))\n",
    "scaler_C = joblib.load(PUBLIC_PT_MODEL_DIR / \"scaler.pkl\")\n",
    "pt_models = []\n",
    "for f in range(5):\n",
    "    checkpoint = torch.load(PUBLIC_PT_MODEL_DIR / f\"gesture_two_branch_fold{f}.pth\", map_location=device)\n",
    "    cfg = {'pad_len': checkpoint['pad_len'], 'imu_dim_raw': checkpoint['imu_dim'], 'tof_dim': checkpoint['tof_dim'], 'n_classes': checkpoint['n_classes']}\n",
    "    m = PublicTwoBranchModel(**cfg).to(device)\n",
    "    m.load_state_dict(checkpoint['model_state_dict'])\n",
    "    m.eval()\n",
    "    pt_models.append(m)\n",
    "print(f\" > Loaded {len(pt_models)} models successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6745342a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:51:08.996996Z",
     "iopub.status.busy": "2025-08-13T05:51:08.996298Z",
     "iopub.status.idle": "2025-08-13T05:51:09.013831Z",
     "shell.execute_reply": "2025-08-13T05:51:09.013268Z"
    },
    "papermill": {
     "duration": 0.024082,
     "end_time": "2025-08-13T05:51:09.014841",
     "exception": false,
     "start_time": "2025-08-13T05:51:08.990759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_3\n",
    "# --- Definition of `predict` function ---\n",
    "def predict3(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    df_seq_orig = sequence.to_pandas()\n",
    "    df_seq_A = df_seq_orig.copy()\n",
    "    linear_accel_A = remove_gravity_from_acc3(df_seq_A[['acc_x','acc_y','acc_z']], df_seq_A[['rot_x','rot_y','rot_z','rot_w']])\n",
    "    df_seq_A['linear_acc_x'], df_seq_A['linear_acc_y'], df_seq_A['linear_acc_z'] = linear_accel_A[:,0], linear_accel_A[:,1], linear_accel_A[:,2]\n",
    "    df_seq_A['linear_acc_mag'] = np.linalg.norm(linear_accel_A, axis=1)\n",
    "    df_seq_A['linear_acc_mag_jerk'] = df_seq_A['linear_acc_mag'].diff().fillna(0)\n",
    "    angular_vel_A = calculate_angular_velocity_from_quat3(df_seq_A[['rot_x','rot_y','rot_z','rot_w']])\n",
    "    df_seq_A['angular_vel_x'], df_seq_A['angular_vel_y'], df_seq_A['angular_vel_z'] = angular_vel_A[:,0], angular_vel_A[:,1], angular_vel_A[:,2]\n",
    "    df_seq_A['angular_distance'] = calculate_angular_distance3(df_seq_A[['rot_x','rot_y','rot_z','rot_w']])\n",
    "    for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']:\n",
    "        df_seq_A[f'{col}_diff'] = df_seq_A[col].diff().fillna(0)\n",
    "    cols_for_stats=['linear_acc_mag','linear_acc_mag_jerk','angular_distance']\n",
    "    for col in cols_for_stats:\n",
    "        df_seq_A[f'{col}_skew'], df_seq_A[f'{col}_kurt'] = df_seq_A[col].skew(), df_seq_A[col].kurtosis()\n",
    "    for i in range(1,6):\n",
    "        if f'tof_{i}_v0' in df_seq_A.columns:\n",
    "            pixel_cols=[f\"tof_{i}_v{p}\" for p in range(64)]; tof_data=df_seq_A[pixel_cols].replace(-1,np.nan)\n",
    "            df_seq_A[f'tof_{i}_mean'], df_seq_A[f'tof_{i}_std'], df_seq_A[f'tof_{i}_min'], df_seq_A[f'tof_{i}_max'] = tof_data.mean(axis=1),tof_data.std(axis=1),tof_data.min(axis=1),tof_data.max(axis=1)\n",
    "    tof_mean_cols=[f'tof_{i}_mean' for i in range(1,6) if f'tof_{i}_mean' in df_seq_A.columns]\n",
    "    if tof_mean_cols:\n",
    "        df_seq_A['tof_std_across_sensors']=df_seq_A[tof_mean_cols].std(axis=1)\n",
    "        df_seq_A['tof_range_across_sensors']=df_seq_A[tof_mean_cols].max(axis=1)-df_seq_A[tof_mean_cols].min(axis=1)\n",
    "    thm_cols=[f'thm_{i}' for i in range(1,6) if f'thm_{i}' in df_seq_A.columns]\n",
    "    if thm_cols:\n",
    "        df_seq_A['thm_std_across_sensors']=df_seq_A[thm_cols].std(axis=1)\n",
    "        df_seq_A['thm_range_across_sensors']=df_seq_A[thm_cols].max(axis=1)-df_seq_A[thm_cols].min(axis=1)\n",
    "    mat_A = df_seq_A[final_feature_cols_A].ffill().bfill().fillna(0).values.astype('float32')\n",
    "    mat_A = scaler_A.transform(mat_A)\n",
    "    pad_input_A = pad_sequences([mat_A], maxlen=pad_len_A, padding='post', dtype='float32')\n",
    "    preds_A_folds = [model.predict(pad_input_A, verbose=0)[0] for model in models_A]\n",
    "    avg_pred_A = np.mean(preds_A_folds, axis=0)\n",
    "    df_seq_B = df_seq_orig.copy()\n",
    "    df_seq_B['acc_mag']=np.sqrt(df_seq_B['acc_x']**2+df_seq_B['acc_y']**2+df_seq_B['acc_z']**2)\n",
    "    df_seq_B['rot_angle']=2*np.arccos(df_seq_B['rot_w'].clip(-1,1))\n",
    "    df_seq_B['acc_mag_jerk']=df_seq_B['acc_mag'].diff().fillna(0)\n",
    "    df_seq_B['rot_angle_vel']=df_seq_B['rot_angle'].diff().fillna(0)\n",
    "    linear_accel_B=remove_gravity_from_acc3(df_seq_B,df_seq_B)\n",
    "    df_seq_B['linear_acc_x'],df_seq_B['linear_acc_y'],df_seq_B['linear_acc_z']=linear_accel_B[:,0],linear_accel_B[:,1],linear_accel_B[:,2]\n",
    "    df_seq_B['linear_acc_mag']=np.sqrt(df_seq_B['linear_acc_x']**2+df_seq_B['linear_acc_y']**2+df_seq_B['linear_acc_z']**2)\n",
    "    df_seq_B['linear_acc_mag_jerk']=df_seq_B['linear_acc_mag'].diff().fillna(0)\n",
    "    angular_vel_B=calculate_angular_velocity_from_quat3(df_seq_B)\n",
    "    df_seq_B['angular_vel_x'],df_seq_B['angular_vel_y'],df_seq_B['angular_vel_z']=angular_vel_B[:,0],angular_vel_B[:,1],angular_vel_B[:,2]\n",
    "    df_seq_B['angular_distance']=calculate_angular_distance3(df_seq_B)\n",
    "    for i in range(1,6):\n",
    "        if f'tof_{i}_v0' in df_seq_B.columns:\n",
    "            pixel_cols=[f\"tof_{i}_v{p}\" for p in range(64)]; tof_data=df_seq_B[pixel_cols].replace(-1,np.nan)\n",
    "            df_seq_B[f\"tof_{i}_mean\"],df_seq_B[f\"tof_{i}_std\"],df_seq_B[f\"tof_{i}_min\"],df_seq_B[f\"tof_{i}_max\"]=tof_data.mean(axis=1),tof_data.std(axis=1),tof_data.min(axis=1),tof_data.max(axis=1)\n",
    "    mat_B = df_seq_B[final_feature_cols_B].ffill().bfill().fillna(0).values.astype('float32')\n",
    "    mat_B = scaler_B.transform(mat_B)\n",
    "    pad_input_B = pad_sequences([mat_B], maxlen=pad_len_B, padding='post', dtype='float32')\n",
    "    pred_B = model_B.predict(pad_input_B, verbose=0)\n",
    "    if isinstance(pred_B, list): pred_B = pred_B[0]\n",
    "    df_seq_C = df_seq_orig.copy() \n",
    "    mat_C = df_seq_C[final_feature_cols_C].ffill().bfill().fillna(0).values.astype('float32')\n",
    "    mat_C = scaler_C.transform(mat_C)\n",
    "    pad_input_C = pad_sequences_torch3([mat_C], maxlen=pad_len_C, padding='pre', truncating='pre')\n",
    "    with torch.no_grad():\n",
    "        pt_input = torch.from_numpy(pad_input_C).to(device)\n",
    "        preds_C_folds = [model(pt_input) for model in pt_models]\n",
    "        avg_pred_C_logits = torch.mean(torch.stack(preds_C_folds), dim=0)\n",
    "        avg_pred_C = torch.softmax(avg_pred_C_logits, dim=1).cpu().numpy()\n",
    "    weights = {'A': 0.50, 'B': 0.20, 'C': 0.30}\n",
    "    final_pred_proba = (weights['A'] * avg_pred_A + weights['B'] * pred_B + weights['C'] * avg_pred_C)\n",
    "    return final_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f5fcf73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:51:09.025296Z",
     "iopub.status.busy": "2025-08-13T05:51:09.025109Z",
     "iopub.status.idle": "2025-08-13T05:51:09.033400Z",
     "shell.execute_reply": "2025-08-13T05:51:09.032679Z"
    },
    "papermill": {
     "duration": 0.015094,
     "end_time": "2025-08-13T05:51:09.034625",
     "exception": false,
     "start_time": "2025-08-13T05:51:09.019531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'w': 0.274, 'p': 1, 'n': 'p0'}, {'w': 0.342, 'p': 2, 'n': 'p1'}, {'w': 0.382, 'p': 3, 'n': 'p2'}] \n",
      "\n",
      " [{'w': 0.382, 'p': 3, 'n': 'p2'}, {'w': 0.342, 'p': 2, 'n': 'p1'}, {'w': 0.274, 'p': 1, 'n': 'p0'}] \n",
      "-----------\n",
      "[{'w': 0.28500000000000003, 'p': 1, 'n': 'p0'}, {'w': 0.338, 'p': 2, 'n': 'p1'}, {'w': 0.375, 'p': 3, 'n': 'p2'}] \n",
      "\n",
      " [{'w': 0.393, 'p': 3, 'n': 'p2'}, {'w': 0.338, 'p': 2, 'n': 'p1'}, {'w': 0.267, 'p': 1, 'n': 'p0'}]\n",
      "-----------\n",
      "[{'w': 0.28500000000000003, 'p': 1, 'n': 'p0'}, {'w': 0.338, 'p': 2, 'n': 'p1'}, {'w': 0.375, 'p': 3, 'n': 'p2'}] \n",
      "\n",
      " [{'w': 0.267, 'p': 1, 'n': 'p0'}, {'w': 0.338, 'p': 2, 'n': 'p1'}, {'w': 0.393, 'p': 3, 'n': 'p2'}]\n",
      "-----------\n",
      "[{'w': 0.28482, 'p': 1, 'n': 'p0'}, {'w': 0.338, 'p': 2, 'n': 'p1'}, {'w': 0.37517999999999996, 'p': 3, 'n': 'p2'}]\n",
      "-----------\n",
      "[0.28482, 0.676, 1.12554]\n"
     ]
    }
   ],
   "source": [
    "pred0,pred1,pred2, ws, cws, aws = 1,2,3, [0.274,0.342,0.382], [+0.011, -0.004, -0.007], [0.99, 0.01]\n",
    "lp = [{ 'w':ws[0], 'p':pred0, 'n':'p0' }, { 'w':ws[1], 'p':pred1, 'n':'p1' }, { 'w':ws[2], 'p':pred2, 'n':'p2' }]\n",
    "lps_asc = [{'w':p['w'], 'p':p['p'], 'n':p['n']} for p in lp]\n",
    "lps_desc = [{'w':p['w'], 'p':p['p'], 'n':p['n']} for p in lp]\n",
    "lps_asc = sorted(lps_asc, key=lambda k:k['p'],reverse=False)\n",
    "lps_desc = sorted(lps_desc, key=lambda k:k['p'],reverse=True)\n",
    "print(lps_asc, \"\\n\\n\", lps_desc, \"\")\n",
    "for p,cw in zip(lps_asc, cws): p['w'] += cw\n",
    "for p,cw in zip(lps_desc, cws): p['w'] += cw\n",
    "print(\"-\"*11)\n",
    "print(lps_asc, \"\\n\\n\", lps_desc)\n",
    "lps_asc = sorted(lps_asc, key=lambda k:k['n'],reverse=False)\n",
    "lps_desc = sorted(lps_desc, key=lambda k:k['n'],reverse=False)\n",
    "print(\"-\"*11)\n",
    "print(lps_asc, \"\\n\\n\", lps_desc)\n",
    "lps = []\n",
    "for a,d in zip(lps_asc, lps_desc):\n",
    "    one_dict = {'w':a['w']* aws[0]+aws[1] *d['w'], 'p':a['p'], 'n':a['n']}\n",
    "    lps.append(one_dict)\n",
    "print(\"-\"*11)\n",
    "print(lps)\n",
    "wps = [ps[\"w\"]*ps[\"p\"] for ps in lps]\n",
    "print(\"-\"*11)\n",
    "print(wps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "453ac970",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:51:09.045004Z",
     "iopub.status.busy": "2025-08-13T05:51:09.044774Z",
     "iopub.status.idle": "2025-08-13T05:51:09.053240Z",
     "shell.execute_reply": "2025-08-13T05:51:09.052650Z"
    },
    "papermill": {
     "duration": 0.014918,
     "end_time": "2025-08-13T05:51:09.054277",
     "exception": false,
     "start_time": "2025-08-13T05:51:09.039359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(sequence, demographics):\n",
    "    import random\n",
    "    pred0 = predict1(sequence, demographics)[0]\n",
    "    pred1 = predict2(sequence, demographics)[0]\n",
    "    pred2 = predict3(sequence, demographics)[0]\n",
    "    wrc21 = np.asarray([0.271, 0.345, 0.384])\n",
    "    wrc22 = np.asarray([0.271, 0.346, 0.383])\n",
    "    wrc23 = np.asarray([0.271, 0.347, 0.382])\n",
    "    wrc24 = np.asarray([0.271, 0.348, 0.381])\n",
    "    wrc25 = np.asarray([0.271, 0.349, 0.380])\n",
    "    wrc28 = np.asarray([0.271, 0.3491, 0.3799])\n",
    "    wrc27 = np.asarray([0.271, 0.3495, 0.3795])\n",
    "    wts = wrc25\n",
    "    c123 = np.asarray([1.00405,0.9974,0.99855])\n",
    "    c132 = np.asarray([1.00405,0.99855,0.9974])\n",
    "    c213 = np.asarray([0.9974,1.00405,0.99855])\n",
    "    c231 = np.asarray([0.99855,1.00405,0.9974])\n",
    "    c312 = np.asarray([0.9974,0.99855,1.00405])\n",
    "    c321 = np.asarray([0.99855,0.9974,1.00405])\n",
    "    r = 5\n",
    "    k = 1.000058\n",
    "    o = 1.0000\n",
    "    def equ(_a,_b,_c,_k=k,o=o):\n",
    "        if _a == _b and _a != _c: return [_k, _k, o]\n",
    "        if _a == _c and _b != _c: return [_k, o, _k]\n",
    "        if _b == _c and _a != _b: return [o, _k, _k]\n",
    "        return [1,1,1]\n",
    "    preds = []\n",
    "    for _a,_b,_c in zip(pred0,pred1,pred2):\n",
    "        a,b,c = round(_a,r),round(_b,r),round(_c,r)\n",
    "        if a <= b <= c: _wts = c123 * wts\n",
    "        elif a <= c <= b: _wts = c132 * wts\n",
    "        elif b <= a <= c: _wts = c213 * wts\n",
    "        elif b <= c <= a: _wts = c231 * wts\n",
    "        elif c <= a <= b: _wts = c312 * wts\n",
    "        elif c <= b <= a: _wts = c321 * wts\n",
    "        _equ = equ(a,b,c)\n",
    "        p = a *_wts[0] *_equ[0] + b *_wts[1] *_equ[1] + c *_wts[2] *_equ[2]\n",
    "        preds.append(p)\n",
    "    avg_pred = np.asarray(preds)\n",
    "    return dataset.le.classes_[avg_pred.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf6d9b89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:51:09.064776Z",
     "iopub.status.busy": "2025-08-13T05:51:09.064577Z",
     "iopub.status.idle": "2025-08-13T05:51:09.071410Z",
     "shell.execute_reply": "2025-08-13T05:51:09.070883Z"
    },
    "papermill": {
     "duration": 0.013408,
     "end_time": "2025-08-13T05:51:09.072513",
     "exception": false,
     "start_time": "2025-08-13T05:51:09.059105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def predict(sequence, demographics):\n",
    "    import copy\n",
    "    pred0 = predict1(sequence, demographics)[0]\n",
    "    pred1 = predict2(sequence, demographics)[0]\n",
    "    pred2 = predict3(sequence, demographics)[0]\n",
    "    preds = []\n",
    "    main_wts = np.asarray([0.271, 0.347, 0.382])\n",
    "    correct_wts = [+0.0021, -0.0007, -0.0014]\n",
    "    asc_desc_wts = [0.70, 0.30]\n",
    "    for a,b,c in zip(pred0,pred1,pred2):\n",
    "        l_abc = [{ 'wts':main_wts[0], 'pred':a, 'n':'p0', 'result':0 }, { 'wts':main_wts[1], 'pred':b, 'n':'p1', 'result':0 }, { 'wts':main_wts[2], 'pred':c, 'n':'p2', 'result':0 }]\n",
    "        lps_asc = sorted(copy.deepcopy(l_abc), key=lambda _:_['pred'],reverse=False)\n",
    "        lps_desc = sorted(copy.deepcopy(l_abc), key=lambda _:_['pred'],reverse=True)\n",
    "        for asc,correct_wt in zip(lps_asc, correct_wts): asc ['wts'] += correct_wt\n",
    "        for desc,correct_wt in zip(lps_desc, correct_wts): desc['wts'] += correct_wt\n",
    "        for asc in lps_asc: asc ['result'] = asc ['pred'] * asc ['wts']\n",
    "        for desc in lps_desc: desc['result'] = desc['pred'] * desc['wts']\n",
    "        result_asc = sum([asc ['result'] for asc in lps_asc])\n",
    "        result_desc = sum([desc['result'] for asc in lps_desc])\n",
    "        result = result_asc * asc_desc_wts[0] + result_desc * asc_desc_wts[1]\n",
    "        preds.append(result)\n",
    "    avg_pred = np.asarray(preds)\n",
    "    return dataset.le.classes_[avg_pred.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60875bbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:51:09.083374Z",
     "iopub.status.busy": "2025-08-13T05:51:09.083164Z",
     "iopub.status.idle": "2025-08-13T05:51:09.086015Z",
     "shell.execute_reply": "2025-08-13T05:51:09.085546Z"
    },
    "papermill": {
     "duration": 0.009524,
     "end_time": "2025-08-13T05:51:09.086953",
     "exception": false,
     "start_time": "2025-08-13T05:51:09.077429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4de1137a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:51:09.097857Z",
     "iopub.status.busy": "2025-08-13T05:51:09.097636Z",
     "iopub.status.idle": "2025-08-13T05:51:43.923930Z",
     "shell.execute_reply": "2025-08-13T05:51:43.923194Z"
    },
    "papermill": {
     "duration": 34.833053,
     "end_time": "2025-08-13T05:51:43.925185",
     "exception": false,
     "start_time": "2025-08-13T05:51:09.092132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 05:51:10.183795: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "I0000 00:00:1755064271.588141      64 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-08-13 05:51:16.030263: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-08-13 05:51:21.044610: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-08-13 05:51:27.054672: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-08-13 05:51:32.395862: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-08-13 05:51:38.086693: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-08-13 05:51:43.090425: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    }
   ],
   "source": [
    "import kaggle_evaluation.cmi_inference_server\n",
    "inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        data_paths=(\n",
    "            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv',\n",
    "            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv',\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89382860",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T05:51:43.937105Z",
     "iopub.status.busy": "2025-08-13T05:51:43.936905Z",
     "iopub.status.idle": "2025-08-13T05:51:44.034322Z",
     "shell.execute_reply": "2025-08-13T05:51:44.033641Z"
    },
    "papermill": {
     "duration": 0.104331,
     "end_time": "2025-08-13T05:51:44.035369",
     "exception": false,
     "start_time": "2025-08-13T05:51:43.931038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sequence_id              gesture\n",
      "0  SEQ_000001  Eyebrow - pull hair\n",
      "1  SEQ_000011  Eyelash - pull hair\n"
     ]
    }
   ],
   "source": [
    "if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    print(pd.read_parquet(\"submission.parquet\"))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12518947,
     "sourceId": 102335,
     "sourceType": "competition"
    },
    {
     "datasetId": 7645099,
     "sourceId": 12139340,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7748073,
     "sourceId": 12293285,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7771623,
     "sourceId": 12328761,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7827797,
     "sourceId": 12411879,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7932089,
     "sourceId": 12573306,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 240649816,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 246893721,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 251413288,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 398856,
     "modelInstanceId": 379625,
     "sourceId": 470587,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 400086,
     "modelInstanceId": 380358,
     "sourceId": 471764,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 235.283525,
   "end_time": "2025-08-13T05:51:47.791481",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-13T05:47:52.507956",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
